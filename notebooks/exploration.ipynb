{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¬ Model Exploration & Training\n",
                "\n",
                "**Vendor Consistency Predictor** â€” XGBoost training pipeline with SHAP interpretability.\n",
                "\n",
                "## Pipeline Overview\n",
                "1. Generate realistic synthetic vendor-order data\n",
                "2. Feature engineering & exploratory analysis\n",
                "3. Train XGBoost regressor with cross-validation\n",
                "4. Evaluate model performance (RMSE, MAE, RÂ²)\n",
                "5. SHAP feature importance analysis\n",
                "6. Export trained model for the FastAPI service"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import xgboost as xgb\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "\n",
                "# Optional: SHAP for model interpretability\n",
                "try:\n",
                "    import shap\n",
                "    SHAP_AVAILABLE = True\n",
                "    print('SHAP loaded successfully')\n",
                "except ImportError:\n",
                "    SHAP_AVAILABLE = False\n",
                "    print('SHAP not installed â€” run: pip install shap')\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "# Plot style\n",
                "sns.set_theme(style='whitegrid', palette='viridis')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "plt.rcParams['figure.dpi'] = 100\n",
                "\n",
                "print('Environment ready âœ…')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Synthetic Data Generation\n",
                "\n",
                "We simulate **10,000 historical orders** across 200 vendors. The target variable `prep_time_minutes` is influenced by:\n",
                "- **Order hour** â€” dinner rush (18â€“21) adds delay\n",
                "- **Day of week** â€” weekends have higher variance\n",
                "- **Item count** â€” more items = longer prep\n",
                "- **Peak hour flag** â€” derived feature for rush periods\n",
                "- **Historical delay average** â€” vendor-specific reliability signal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N_ORDERS = 10_000\n",
                "N_VENDORS = 200\n",
                "\n",
                "# Generate vendor-level characteristics (some vendors are consistently slow)\n",
                "vendor_base_delay = {vid: np.random.exponential(3.0) for vid in range(1, N_VENDORS + 1)}\n",
                "\n",
                "# Generate order-level data\n",
                "data = []\n",
                "for _ in range(N_ORDERS):\n",
                "    vendor_id = np.random.randint(1, N_VENDORS + 1)\n",
                "    order_hour = np.random.choice(range(24), p=[\n",
                "        0.01, 0.005, 0.005, 0.005, 0.005, 0.01, 0.02, 0.03,  # 0-7\n",
                "        0.04, 0.04, 0.05, 0.07, 0.09, 0.07, 0.05, 0.04,      # 8-15\n",
                "        0.05, 0.06, 0.08, 0.09, 0.07, 0.05, 0.03, 0.02       # 16-23\n",
                "    ])\n",
                "    day_of_week = np.random.randint(0, 7)\n",
                "    item_count = np.random.randint(1, 12)\n",
                "    is_peak_hour = 1 if order_hour in [12, 13, 18, 19, 20, 21] else 0\n",
                "    historical_delay_avg = vendor_base_delay[vendor_id] + np.random.normal(0, 1)\n",
                "    historical_delay_avg = max(0, historical_delay_avg)\n",
                "\n",
                "    # Target: prep time with realistic dependencies\n",
                "    prep_time = (\n",
                "        8.0                                           # base prep time\n",
                "        + item_count * 1.8                            # per-item cost\n",
                "        + is_peak_hour * np.random.uniform(2, 6)      # rush hour penalty\n",
                "        + historical_delay_avg * 0.7                  # vendor reliability\n",
                "        + (1 if day_of_week >= 5 else 0) * 2.5        # weekend surge\n",
                "        + np.random.normal(0, 3)                      # noise\n",
                "    )\n",
                "    prep_time = max(3, prep_time)  # minimum 3 minutes\n",
                "\n",
                "    data.append({\n",
                "        'vendor_id': vendor_id,\n",
                "        'order_hour': order_hour,\n",
                "        'day_of_week': day_of_week,\n",
                "        'item_count': item_count,\n",
                "        'is_peak_hour': is_peak_hour,\n",
                "        'historical_delay_avg': round(historical_delay_avg, 2),\n",
                "        'prep_time_minutes': round(prep_time, 2)\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "print(f'Dataset: {df.shape[0]:,} orders across {df[\"vendor_id\"].nunique()} vendors')\n",
                "df.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe().round(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Distribution of prep time\n",
                "axes[0, 0].hist(df['prep_time_minutes'], bins=50, edgecolor='black', alpha=0.7)\n",
                "axes[0, 0].set_title('Distribution of Prep Time')\n",
                "axes[0, 0].set_xlabel('Prep Time (minutes)')\n",
                "axes[0, 0].axvline(df['prep_time_minutes'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"prep_time_minutes\"].mean():.1f} min')\n",
                "axes[0, 0].legend()\n",
                "\n",
                "# Prep time by hour\n",
                "hourly = df.groupby('order_hour')['prep_time_minutes'].mean()\n",
                "axes[0, 1].bar(hourly.index, hourly.values, color=sns.color_palette('viridis', len(hourly)))\n",
                "axes[0, 1].set_title('Avg Prep Time by Hour')\n",
                "axes[0, 1].set_xlabel('Order Hour')\n",
                "axes[0, 1].set_ylabel('Avg Prep Time (min)')\n",
                "\n",
                "# Prep time vs item count\n",
                "axes[1, 0].scatter(df['item_count'], df['prep_time_minutes'], alpha=0.1, s=5)\n",
                "item_means = df.groupby('item_count')['prep_time_minutes'].mean()\n",
                "axes[1, 0].plot(item_means.index, item_means.values, color='red', linewidth=2, marker='o')\n",
                "axes[1, 0].set_title('Prep Time vs Item Count')\n",
                "axes[1, 0].set_xlabel('Item Count')\n",
                "axes[1, 0].set_ylabel('Prep Time (min)')\n",
                "\n",
                "# Correlation heatmap\n",
                "corr = df[['order_hour', 'day_of_week', 'item_count', 'is_peak_hour', 'historical_delay_avg', 'prep_time_minutes']].corr()\n",
                "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 1], center=0)\n",
                "axes[1, 1].set_title('Feature Correlation Matrix')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('eda_overview.png', bbox_inches='tight')\n",
                "plt.show()\n",
                "print('Saved: eda_overview.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Train/Test Split & XGBoost Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FEATURE_COLS = ['order_hour', 'day_of_week', 'item_count', 'is_peak_hour', 'historical_delay_avg']\n",
                "TARGET = 'prep_time_minutes'\n",
                "\n",
                "X = df[FEATURE_COLS]\n",
                "y = df[TARGET]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "print(f'Train: {X_train.shape[0]:,} samples')\n",
                "print(f'Test:  {X_test.shape[0]:,} samples')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost with tuned hyperparameters\n",
                "params = {\n",
                "    'objective': 'reg:squarederror',\n",
                "    'max_depth': 6,\n",
                "    'learning_rate': 0.1,\n",
                "    'n_estimators': 200,\n",
                "    'subsample': 0.8,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'min_child_weight': 5,\n",
                "    'reg_alpha': 0.1,\n",
                "    'reg_lambda': 1.0,\n",
                "    'random_state': 42,\n",
                "    'n_jobs': -1\n",
                "}\n",
                "\n",
                "model = xgb.XGBRegressor(**params)\n",
                "model.fit(\n",
                "    X_train, y_train,\n",
                "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
                "    verbose=20\n",
                ")\n",
                "\n",
                "print('\\nTraining complete âœ…')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict(X_test)\n",
                "\n",
                "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print('=' * 45)\n",
                "print('           MODEL EVALUATION RESULTS')\n",
                "print('=' * 45)\n",
                "print(f'  RMSE:  {rmse:.2f} minutes')\n",
                "print(f'  MAE:   {mae:.2f} minutes')\n",
                "print(f'  RÂ²:    {r2:.4f}')\n",
                "print('=' * 45)\n",
                "\n",
                "# Cross-validation for robustness check\n",
                "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
                "print(f'\\n5-Fold CV RMSE: {-cv_scores.mean():.2f} Â± {cv_scores.std():.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predicted vs Actual scatter\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Scatter plot\n",
                "axes[0].scatter(y_test, y_pred, alpha=0.3, s=10, c='steelblue')\n",
                "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect prediction')\n",
                "axes[0].set_xlabel('Actual Prep Time (min)')\n",
                "axes[0].set_ylabel('Predicted Prep Time (min)')\n",
                "axes[0].set_title(f'Predicted vs Actual  (RÂ² = {r2:.3f})')\n",
                "axes[0].legend()\n",
                "\n",
                "# Residual distribution\n",
                "residuals = y_test - y_pred\n",
                "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
                "axes[1].axvline(0, color='black', linestyle='--')\n",
                "axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
                "axes[1].set_ylabel('Count')\n",
                "axes[1].set_title(f'Residual Distribution  (MAE = {mae:.2f} min)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('model_evaluation.png', bbox_inches='tight')\n",
                "plt.show()\n",
                "print('Saved: model_evaluation.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. SHAP Feature Importance\n",
                "\n",
                "SHAP (SHapley Additive exPlanations) provides model-agnostic, per-prediction feature attributions.\n",
                "This is critical for stakeholder buy-in â€” they need to see **why** the model predicts a delay."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if SHAP_AVAILABLE:\n",
                "    explainer = shap.TreeExplainer(model)\n",
                "    shap_values = explainer.shap_values(X_test)\n",
                "\n",
                "    # Summary plot â€” global feature importance with direction\n",
                "    print('SHAP Summary Plot â€” which features drive predictions?')\n",
                "    shap.summary_plot(shap_values, X_test, show=False)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('shap_summary.png', bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print('Saved: shap_summary.png')\n",
                "else:\n",
                "    # Fallback: XGBoost built-in feature importance\n",
                "    print('Using XGBoost built-in feature importance (install shap for SHAP plots)')\n",
                "    importance = model.feature_importances_\n",
                "    feat_imp = pd.Series(importance, index=FEATURE_COLS).sort_values(ascending=True)\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(8, 5))\n",
                "    feat_imp.plot(kind='barh', ax=ax, color=sns.color_palette('viridis', len(feat_imp)))\n",
                "    ax.set_xlabel('Feature Importance (Gain)')\n",
                "    ax.set_title('XGBoost Feature Importance')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('feature_importance.png', bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print('Saved: feature_importance.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if SHAP_AVAILABLE:\n",
                "    # Single prediction explanation â€” useful for debugging individual predictions\n",
                "    sample_idx = 0\n",
                "    print(f'Explaining prediction for test sample #{sample_idx}:')\n",
                "    print(f'  Actual: {y_test.iloc[sample_idx]:.1f} min')\n",
                "    print(f'  Predicted: {y_pred[sample_idx]:.1f} min')\n",
                "    print()\n",
                "    \n",
                "    shap.force_plot(explainer.expected_value, shap_values[sample_idx], X_test.iloc[sample_idx], matplotlib=True)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('shap_force_plot.png', bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print('Saved: shap_force_plot.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Export Model for FastAPI Service\n",
                "\n",
                "Save the trained model in XGBoost's native JSON format. The FastAPI service (`app/model.py`) loads this file at startup."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "MODEL_PATH = os.path.join('..', 'model.json')\n",
                "\n",
                "# Save using XGBoost's Booster (matches the loading logic in app/model.py)\n",
                "model.get_booster().save_model(MODEL_PATH)\n",
                "\n",
                "# Verify the saved model loads correctly\n",
                "booster = xgb.Booster()\n",
                "booster.load_model(MODEL_PATH)\n",
                "test_dmatrix = xgb.DMatrix(X_test.iloc[:1])\n",
                "verify_pred = booster.predict(test_dmatrix)\n",
                "\n",
                "print(f'Model saved to: {os.path.abspath(MODEL_PATH)}')\n",
                "print(f'Model size: {os.path.getsize(MODEL_PATH) / 1024:.1f} KB')\n",
                "print(f'Verification prediction: {verify_pred[0]:.2f} min')\n",
                "print(f'Original prediction:     {y_pred[0]:.2f} min')\n",
                "print(f'Match: {np.isclose(verify_pred[0], y_pred[0])} âœ…')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "| Metric | Value |\n",
                "|--------|-------|\n",
                "| Training samples | 8,000 |\n",
                "| Test samples | 2,000 |\n",
                "| Features | 5 |\n",
                "| Model | XGBoost Regressor |\n",
                "| RMSE | See output above |\n",
                "| RÂ² | See output above |\n",
                "\n",
                "**Key findings:**\n",
                "- `item_count` and `historical_delay_avg` are the strongest predictors\n",
                "- Peak hour effects are captured by `is_peak_hour`\n",
                "- Weekend surge adds ~2.5 min on average\n",
                "\n",
                "**Next steps:**\n",
                "- Integrate with a real feature store (BigQuery â†’ `data/features.sql`)\n",
                "- Add confidence intervals via quantile regression\n",
                "- Experiment with LightGBM / CatBoost for comparison\n",
                "- Deploy model monitoring with prediction drift detection"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}